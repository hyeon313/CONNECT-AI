{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D, LeakyReLU\n",
    "from tensorflow.keras.layers import  Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import voxel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def dataload():\n",
    "    img = np.load(\"imgs.npy\")\n",
    "    mask = np.load(\"masks.npy\")\n",
    "\n",
    "\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1) \n",
    "#     mask = to_categorical(mask) \n",
    "    \n",
    "    return img, mask \n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    yt0 = y_true[:,:,:,0]\n",
    "    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
    "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
    "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
    "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
    "    return iou\n",
    "\n",
    "# def minmaxScaler(data):\n",
    "\n",
    "# def dice_coef(y_true, y_pred, smooth=1):\n",
    "#     y_pred = K.argmax(y_pred, axis=-1)\n",
    "# #     y_true = y_true[:,:,:,0]\n",
    "\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     y_true_f = K.cast(y_true_f, 'float32')\n",
    "#     y_pred_f = K.cast(y_pred_f, 'float32')\n",
    "\n",
    "#     intersection = K.sum(y_true_f * y_pred_f)\n",
    "#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_multilabel(y_true, y_pred, numLabels=3):\n",
    "    dice = 0\n",
    "\n",
    "    for index in range(numLabels):\n",
    "        if index == 0: continue\n",
    "        dice += dice_coef_each(y_true, y_pred, index)\n",
    "        \n",
    "    return dice / numLabels # taking average\n",
    "\n",
    "def dice_coef_each(y_true, y_pred, label, smooth=1):\n",
    "    y_true = K.cast(K.equal(y_true, label), 'float32')\n",
    "    y_pred = K.cast(K.equal(K.argmax(y_pred, axis=-1), label), 'float32')\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_0(y_true, y_pred):\n",
    "    return dice_coef_each(y_true, y_pred, 0)\n",
    "\n",
    "def dice_coef_1(y_true, y_pred):\n",
    "    return dice_coef_each(y_true, y_pred, 1)\n",
    "\n",
    "def dice_coef_2(y_true, y_pred):\n",
    "    return dice_coef_each(y_true, y_pred, 2)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = y_true[:,:,:,1]\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    y_true_f = K.cast(y_true_f, 'float32')\n",
    "    y_pred_f = K.cast(y_pred_f, 'float32')\n",
    "    \n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return tf.math.exp(1  - score) - 1.0\n",
    "    # return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def build_unet(sz=(512,512,1)):\n",
    "    x = Input(sz)\n",
    "    inputs = x\n",
    "  \n",
    "    #down sampling \n",
    "    f = 8\n",
    "    layers = []\n",
    "  \n",
    "    for i in range(0, 6):\n",
    "        # kernel_initializer='he_norm' kernel의 값을 맞춰줄 수 있음. (초기화 설정) he_norm 앞 레이어의 평균과 표준편차를 맞춰서 정규화를 해준다.\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "        # bias는 BatchNormalization에서 조절\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', use_bias=False, kernel_initializer='he_normal') (x)\n",
    "        # BatchNormalization\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        layers.append(x)\n",
    "        x = MaxPooling2D() (x)\n",
    "        f = f*2\n",
    "        ff2 = 64 \n",
    "    \n",
    "    #bottleneck \n",
    "    j = len(layers) - 1\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
    "    x = Concatenate(axis=3)([x, layers[j]])\n",
    "    j = j -1 \n",
    "  \n",
    "    #upsampling \n",
    "    for i in range(0, 5):\n",
    "        ff2 = ff2//2\n",
    "        f = f // 2 \n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "        x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same', use_bias=False) (x)\n",
    "        # BatchNormalization\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = Concatenate(axis=3)([x, layers[j]])\n",
    "        j = j -1 \n",
    "    \n",
    "    #classification \n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    outputs = Conv2D(3, 1, activation='softmax') (x)\n",
    "    \n",
    "    #model creation \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', \n",
    "                  metrics = [dice_coef_0, dice_coef_1, dice_coef_2])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3890, 512, 512, 1) (3890, 512, 512, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 8)  80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 8)  576         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512, 512, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 512, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 256, 256, 8)  0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 16) 1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 16) 2304        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 16) 64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256, 256, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 16) 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 32) 4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 32) 9216        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 32) 128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 32)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   36864       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 64)   0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 128)  147456      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 128)  0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  295168      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  589824      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 16, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 256)    0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 512)    1180160     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 8, 8, 512)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 64)   131136      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 320)  0           conv2d_transpose[0][0]           \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 256)  737536      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 32)   32768       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 160)  0           dropout_7[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  184448      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 16)   8192        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 16)   64          conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 64, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 80)   0           dropout_8[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 64)   46144       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 8)  2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 8)  32          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 128, 8)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 40) 0           dropout_9[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 32) 11552       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 32) 9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 4)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 256, 4)  16          conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256, 256, 4)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256, 256, 20) 0           dropout_10[0][0]                 \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 16) 2896        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 16) 2320        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 512, 512, 2)  128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512, 512, 2)  8           conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512, 512, 2)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512, 512, 10) 0           dropout_11[0][0]                 \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 512, 512, 16) 1456        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 512, 512, 16) 2320        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 512, 512, 16) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 512, 512, 3)  51          dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,669,227\n",
      "Trainable params: 6,668,095\n",
      "Non-trainable params: 1,132\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "389/389 [==============================] - 59s 151ms/step - loss: 0.0875 - dice_coef_0: 0.9857 - dice_coef_1: 0.3360 - dice_coef_2: 0.3899 - val_loss: 0.0610 - val_dice_coef_0: 0.9851 - val_dice_coef_1: 0.2381 - val_dice_coef_2: 0.2478\n",
      "Epoch 2/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0206 - dice_coef_0: 0.9976 - dice_coef_1: 0.7865 - dice_coef_2: 0.7842 - val_loss: 0.0351 - val_dice_coef_0: 0.9934 - val_dice_coef_1: 0.2718 - val_dice_coef_2: 0.3630\n",
      "Epoch 3/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0164 - dice_coef_0: 0.9983 - dice_coef_1: 0.8267 - dice_coef_2: 0.8114 - val_loss: 0.0342 - val_dice_coef_0: 0.9945 - val_dice_coef_1: 0.4542 - val_dice_coef_2: 0.6111\n",
      "Epoch 4/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0130 - dice_coef_0: 0.9987 - dice_coef_1: 0.8262 - dice_coef_2: 0.8057 - val_loss: 0.0174 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.9312 - val_dice_coef_2: 0.9463\n",
      "Epoch 5/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0119 - dice_coef_0: 0.9988 - dice_coef_1: 0.8413 - dice_coef_2: 0.8288 - val_loss: 0.0168 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.8374 - val_dice_coef_2: 0.9375\n",
      "Epoch 6/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0117 - dice_coef_0: 0.9988 - dice_coef_1: 0.8425 - dice_coef_2: 0.8216 - val_loss: 0.0157 - val_dice_coef_0: 0.9983 - val_dice_coef_1: 0.5970 - val_dice_coef_2: 0.8860\n",
      "Epoch 7/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0127 - dice_coef_0: 0.9986 - dice_coef_1: 0.8424 - dice_coef_2: 0.8229 - val_loss: 0.0310 - val_dice_coef_0: 0.9971 - val_dice_coef_1: 0.5481 - val_dice_coef_2: 0.6810\n",
      "Epoch 8/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0102 - dice_coef_0: 0.9989 - dice_coef_1: 0.8701 - dice_coef_2: 0.8538 - val_loss: 0.0229 - val_dice_coef_0: 0.9977 - val_dice_coef_1: 0.6544 - val_dice_coef_2: 0.8188\n",
      "Epoch 9/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0099 - dice_coef_0: 0.9990 - dice_coef_1: 0.8723 - dice_coef_2: 0.8470 - val_loss: 0.1481 - val_dice_coef_0: 0.9895 - val_dice_coef_1: 0.2918 - val_dice_coef_2: 0.4187\n",
      "Epoch 10/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0093 - dice_coef_0: 0.9991 - dice_coef_1: 0.8801 - dice_coef_2: 0.8719 - val_loss: 0.0151 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.6711 - val_dice_coef_2: 0.8675\n",
      "Epoch 11/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0084 - dice_coef_0: 0.9991 - dice_coef_1: 0.8910 - dice_coef_2: 0.8933 - val_loss: 0.0208 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.8062 - val_dice_coef_2: 0.9410\n",
      "Epoch 12/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0085 - dice_coef_0: 0.9991 - dice_coef_1: 0.8821 - dice_coef_2: 0.8948 - val_loss: 0.0159 - val_dice_coef_0: 0.9987 - val_dice_coef_1: 0.8510 - val_dice_coef_2: 0.9415\n",
      "Epoch 13/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0089 - dice_coef_0: 0.9991 - dice_coef_1: 0.8821 - dice_coef_2: 0.8786 - val_loss: 0.0184 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.7341 - val_dice_coef_2: 0.8487\n",
      "Epoch 14/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0077 - dice_coef_0: 0.9992 - dice_coef_1: 0.8858 - dice_coef_2: 0.8960 - val_loss: 0.0229 - val_dice_coef_0: 0.9977 - val_dice_coef_1: 0.7656 - val_dice_coef_2: 0.8861\n",
      "Epoch 15/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0078 - dice_coef_0: 0.9992 - dice_coef_1: 0.8893 - dice_coef_2: 0.8961 - val_loss: 0.0163 - val_dice_coef_0: 0.9983 - val_dice_coef_1: 0.7445 - val_dice_coef_2: 0.8933\n",
      "Epoch 16/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0081 - dice_coef_0: 0.9991 - dice_coef_1: 0.8872 - dice_coef_2: 0.9006 - val_loss: 0.0102 - val_dice_coef_0: 0.9990 - val_dice_coef_1: 0.7918 - val_dice_coef_2: 0.9492\n",
      "Epoch 17/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0071 - dice_coef_0: 0.9993 - dice_coef_1: 0.8857 - dice_coef_2: 0.8942 - val_loss: 0.0183 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.7120 - val_dice_coef_2: 0.8008\n",
      "Epoch 18/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0073 - dice_coef_0: 0.9992 - dice_coef_1: 0.8976 - dice_coef_2: 0.9139 - val_loss: 0.0198 - val_dice_coef_0: 0.9986 - val_dice_coef_1: 0.5311 - val_dice_coef_2: 0.7400\n",
      "Epoch 19/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0070 - dice_coef_0: 0.9993 - dice_coef_1: 0.9048 - dice_coef_2: 0.9136 - val_loss: 0.0217 - val_dice_coef_0: 0.9978 - val_dice_coef_1: 0.7464 - val_dice_coef_2: 0.8783\n",
      "Epoch 20/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0067 - dice_coef_0: 0.9993 - dice_coef_1: 0.9151 - dice_coef_2: 0.9242 - val_loss: 0.0144 - val_dice_coef_0: 0.9987 - val_dice_coef_1: 0.7801 - val_dice_coef_2: 0.9206\n",
      "Epoch 21/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0072 - dice_coef_0: 0.9992 - dice_coef_1: 0.9062 - dice_coef_2: 0.9080 - val_loss: 0.0148 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.5670 - val_dice_coef_2: 0.7315\n",
      "Epoch 22/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0066 - dice_coef_0: 0.9993 - dice_coef_1: 0.9140 - dice_coef_2: 0.9252 - val_loss: 0.0343 - val_dice_coef_0: 0.9971 - val_dice_coef_1: 0.5645 - val_dice_coef_2: 0.8565\n",
      "Epoch 23/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0067 - dice_coef_0: 0.9993 - dice_coef_1: 0.9078 - dice_coef_2: 0.9173 - val_loss: 0.0190 - val_dice_coef_0: 0.9984 - val_dice_coef_1: 0.6940 - val_dice_coef_2: 0.8655\n",
      "Epoch 24/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0069 - dice_coef_0: 0.9993 - dice_coef_1: 0.8999 - dice_coef_2: 0.9132 - val_loss: 0.0205 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.7003 - val_dice_coef_2: 0.7933\n",
      "Epoch 25/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0063 - dice_coef_0: 0.9993 - dice_coef_1: 0.9169 - dice_coef_2: 0.9285 - val_loss: 0.0154 - val_dice_coef_0: 0.9984 - val_dice_coef_1: 0.7535 - val_dice_coef_2: 0.9154\n",
      "Epoch 26/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0061 - dice_coef_0: 0.9994 - dice_coef_1: 0.9029 - dice_coef_2: 0.9306 - val_loss: 0.0161 - val_dice_coef_0: 0.9984 - val_dice_coef_1: 0.7368 - val_dice_coef_2: 0.9003\n",
      "Epoch 27/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0059 - dice_coef_0: 0.9994 - dice_coef_1: 0.9224 - dice_coef_2: 0.9354 - val_loss: 0.0210 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.6807 - val_dice_coef_2: 0.9138\n",
      "Epoch 28/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0061 - dice_coef_0: 0.9993 - dice_coef_1: 0.8976 - dice_coef_2: 0.9367 - val_loss: 0.0113 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.7507 - val_dice_coef_2: 0.9383\n",
      "Epoch 29/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0064 - dice_coef_0: 0.9993 - dice_coef_1: 0.9021 - dice_coef_2: 0.9288 - val_loss: 0.0161 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.6269 - val_dice_coef_2: 0.8891\n",
      "Epoch 30/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0058 - dice_coef_0: 0.9994 - dice_coef_1: 0.9141 - dice_coef_2: 0.9329 - val_loss: 0.0284 - val_dice_coef_0: 0.9973 - val_dice_coef_1: 0.6692 - val_dice_coef_2: 0.8377\n",
      "Epoch 31/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0056 - dice_coef_0: 0.9994 - dice_coef_1: 0.9041 - dice_coef_2: 0.9380 - val_loss: 0.0272 - val_dice_coef_0: 0.9977 - val_dice_coef_1: 0.7443 - val_dice_coef_2: 0.8957\n",
      "Epoch 32/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0070 - dice_coef_0: 0.9993 - dice_coef_1: 0.9019 - dice_coef_2: 0.9135 - val_loss: 0.0187 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.7170 - val_dice_coef_2: 0.9276\n",
      "Epoch 33/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0056 - dice_coef_0: 0.9994 - dice_coef_1: 0.9178 - dice_coef_2: 0.9365 - val_loss: 0.0238 - val_dice_coef_0: 0.9975 - val_dice_coef_1: 0.7085 - val_dice_coef_2: 0.9050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0054 - dice_coef_0: 0.9994 - dice_coef_1: 0.9059 - dice_coef_2: 0.9379 - val_loss: 0.0173 - val_dice_coef_0: 0.9983 - val_dice_coef_1: 0.7652 - val_dice_coef_2: 0.9388\n",
      "Epoch 35/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0051 - dice_coef_0: 0.9994 - dice_coef_1: 0.9253 - dice_coef_2: 0.9348 - val_loss: 0.0226 - val_dice_coef_0: 0.9978 - val_dice_coef_1: 0.7054 - val_dice_coef_2: 0.9280\n",
      "Epoch 36/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0055 - dice_coef_0: 0.9994 - dice_coef_1: 0.9135 - dice_coef_2: 0.9278 - val_loss: 0.0220 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.7848 - val_dice_coef_2: 0.9341\n",
      "Epoch 37/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0053 - dice_coef_0: 0.9994 - dice_coef_1: 0.9149 - dice_coef_2: 0.9384 - val_loss: 0.0197 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.7207 - val_dice_coef_2: 0.9215\n",
      "Epoch 38/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0051 - dice_coef_0: 0.9995 - dice_coef_1: 0.9224 - dice_coef_2: 0.9399 - val_loss: 0.0233 - val_dice_coef_0: 0.9979 - val_dice_coef_1: 0.6962 - val_dice_coef_2: 0.9158\n",
      "Epoch 39/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0051 - dice_coef_0: 0.9995 - dice_coef_1: 0.9156 - dice_coef_2: 0.9474 - val_loss: 0.0236 - val_dice_coef_0: 0.9976 - val_dice_coef_1: 0.7083 - val_dice_coef_2: 0.9363\n",
      "Epoch 40/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0049 - dice_coef_0: 0.9995 - dice_coef_1: 0.9278 - dice_coef_2: 0.9481 - val_loss: 0.0218 - val_dice_coef_0: 0.9979 - val_dice_coef_1: 0.7510 - val_dice_coef_2: 0.9526\n",
      "Epoch 41/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0049 - dice_coef_0: 0.9995 - dice_coef_1: 0.9152 - dice_coef_2: 0.9425 - val_loss: 0.0265 - val_dice_coef_0: 0.9975 - val_dice_coef_1: 0.7262 - val_dice_coef_2: 0.9191\n",
      "Epoch 42/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0049 - dice_coef_0: 0.9995 - dice_coef_1: 0.9203 - dice_coef_2: 0.9499 - val_loss: 0.0296 - val_dice_coef_0: 0.9972 - val_dice_coef_1: 0.6768 - val_dice_coef_2: 0.9209\n",
      "Epoch 43/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0048 - dice_coef_0: 0.9995 - dice_coef_1: 0.9172 - dice_coef_2: 0.9438 - val_loss: 0.0598 - val_dice_coef_0: 0.9947 - val_dice_coef_1: 0.4594 - val_dice_coef_2: 0.7415\n",
      "Epoch 44/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0052 - dice_coef_0: 0.9994 - dice_coef_1: 0.9093 - dice_coef_2: 0.9428 - val_loss: 0.0224 - val_dice_coef_0: 0.9979 - val_dice_coef_1: 0.6483 - val_dice_coef_2: 0.9126\n",
      "Epoch 45/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0048 - dice_coef_0: 0.9995 - dice_coef_1: 0.9247 - dice_coef_2: 0.9514 - val_loss: 0.0170 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.7866 - val_dice_coef_2: 0.9520\n",
      "Epoch 46/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0047 - dice_coef_0: 0.9995 - dice_coef_1: 0.9104 - dice_coef_2: 0.9503 - val_loss: 0.0230 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.7496 - val_dice_coef_2: 0.9344\n",
      "Epoch 47/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0058 - dice_coef_0: 0.9994 - dice_coef_1: 0.9163 - dice_coef_2: 0.9480 - val_loss: 0.0358 - val_dice_coef_0: 0.9974 - val_dice_coef_1: 0.4934 - val_dice_coef_2: 0.5937\n",
      "Epoch 48/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0068 - dice_coef_0: 0.9993 - dice_coef_1: 0.9060 - dice_coef_2: 0.9237 - val_loss: 0.0077 - val_dice_coef_0: 0.9992 - val_dice_coef_1: 0.7815 - val_dice_coef_2: 0.9648\n",
      "Epoch 49/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0050 - dice_coef_0: 0.9995 - dice_coef_1: 0.9077 - dice_coef_2: 0.9418 - val_loss: 0.0118 - val_dice_coef_0: 0.9986 - val_dice_coef_1: 0.6979 - val_dice_coef_2: 0.9596\n",
      "Epoch 50/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0046 - dice_coef_0: 0.9995 - dice_coef_1: 0.9061 - dice_coef_2: 0.9462 - val_loss: 0.0117 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.7174 - val_dice_coef_2: 0.9487\n",
      "Epoch 51/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0047 - dice_coef_0: 0.9995 - dice_coef_1: 0.9294 - dice_coef_2: 0.9463 - val_loss: 0.0386 - val_dice_coef_0: 0.9966 - val_dice_coef_1: 0.4007 - val_dice_coef_2: 0.6442\n",
      "Epoch 52/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0047 - dice_coef_0: 0.9995 - dice_coef_1: 0.9168 - dice_coef_2: 0.9451 - val_loss: 0.0090 - val_dice_coef_0: 0.9991 - val_dice_coef_1: 0.8354 - val_dice_coef_2: 0.9633\n",
      "Epoch 53/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0045 - dice_coef_0: 0.9995 - dice_coef_1: 0.9261 - dice_coef_2: 0.9493 - val_loss: 0.0134 - val_dice_coef_0: 0.9986 - val_dice_coef_1: 0.7156 - val_dice_coef_2: 0.9565\n",
      "Epoch 54/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0045 - dice_coef_0: 0.9995 - dice_coef_1: 0.9288 - dice_coef_2: 0.9512 - val_loss: 0.0146 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.7491 - val_dice_coef_2: 0.9433\n",
      "Epoch 55/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0044 - dice_coef_0: 0.9995 - dice_coef_1: 0.9370 - dice_coef_2: 0.9438 - val_loss: 0.0145 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.7598 - val_dice_coef_2: 0.9428\n",
      "Epoch 56/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0045 - dice_coef_0: 0.9995 - dice_coef_1: 0.9262 - dice_coef_2: 0.9530 - val_loss: 0.0374 - val_dice_coef_0: 0.9975 - val_dice_coef_1: 0.7791 - val_dice_coef_2: 0.9559\n",
      "Epoch 57/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0044 - dice_coef_0: 0.9995 - dice_coef_1: 0.9174 - dice_coef_2: 0.9537 - val_loss: 0.0370 - val_dice_coef_0: 0.9971 - val_dice_coef_1: 0.6809 - val_dice_coef_2: 0.9410\n",
      "Epoch 58/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0043 - dice_coef_0: 0.9995 - dice_coef_1: 0.9283 - dice_coef_2: 0.9554 - val_loss: 0.0438 - val_dice_coef_0: 0.9967 - val_dice_coef_1: 0.6311 - val_dice_coef_2: 0.8934\n",
      "Epoch 59/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0044 - dice_coef_0: 0.9995 - dice_coef_1: 0.9112 - dice_coef_2: 0.9527 - val_loss: 0.0285 - val_dice_coef_0: 0.9977 - val_dice_coef_1: 0.7233 - val_dice_coef_2: 0.9430\n",
      "Epoch 60/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0043 - dice_coef_0: 0.9995 - dice_coef_1: 0.9191 - dice_coef_2: 0.9513 - val_loss: 0.0298 - val_dice_coef_0: 0.9975 - val_dice_coef_1: 0.7121 - val_dice_coef_2: 0.9431\n",
      "Epoch 61/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0043 - dice_coef_0: 0.9995 - dice_coef_1: 0.9279 - dice_coef_2: 0.9531 - val_loss: 0.0290 - val_dice_coef_0: 0.9974 - val_dice_coef_1: 0.5931 - val_dice_coef_2: 0.8940\n",
      "Epoch 62/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0042 - dice_coef_0: 0.9995 - dice_coef_1: 0.9293 - dice_coef_2: 0.9493 - val_loss: 0.0633 - val_dice_coef_0: 0.9954 - val_dice_coef_1: 0.5348 - val_dice_coef_2: 0.8140\n",
      "Epoch 63/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0042 - dice_coef_0: 0.9995 - dice_coef_1: 0.9366 - dice_coef_2: 0.9581 - val_loss: 0.0536 - val_dice_coef_0: 0.9962 - val_dice_coef_1: 0.6381 - val_dice_coef_2: 0.9228\n",
      "Epoch 64/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0042 - dice_coef_0: 0.9995 - dice_coef_1: 0.9320 - dice_coef_2: 0.9582 - val_loss: 0.0485 - val_dice_coef_0: 0.9964 - val_dice_coef_1: 0.7124 - val_dice_coef_2: 0.9033\n",
      "Epoch 65/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0043 - dice_coef_0: 0.9995 - dice_coef_1: 0.9269 - dice_coef_2: 0.9584 - val_loss: 0.0384 - val_dice_coef_0: 0.9973 - val_dice_coef_1: 0.6103 - val_dice_coef_2: 0.9312\n",
      "Epoch 66/100\n",
      "389/389 [==============================] - 57s 147ms/step - loss: 0.0042 - dice_coef_0: 0.9995 - dice_coef_1: 0.9225 - dice_coef_2: 0.9517 - val_loss: 0.0268 - val_dice_coef_0: 0.9978 - val_dice_coef_1: 0.7002 - val_dice_coef_2: 0.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0041 - dice_coef_0: 0.9995 - dice_coef_1: 0.9265 - dice_coef_2: 0.9523 - val_loss: 0.0508 - val_dice_coef_0: 0.9966 - val_dice_coef_1: 0.6695 - val_dice_coef_2: 0.9086\n",
      "Epoch 68/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0041 - dice_coef_0: 0.9995 - dice_coef_1: 0.9352 - dice_coef_2: 0.9564 - val_loss: 0.0293 - val_dice_coef_0: 0.9979 - val_dice_coef_1: 0.7188 - val_dice_coef_2: 0.9148\n",
      "Epoch 69/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0057 - dice_coef_0: 0.9994 - dice_coef_1: 0.9147 - dice_coef_2: 0.9428 - val_loss: 0.0161 - val_dice_coef_0: 0.9984 - val_dice_coef_1: 0.6650 - val_dice_coef_2: 0.9058\n",
      "Epoch 70/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0046 - dice_coef_0: 0.9995 - dice_coef_1: 0.9113 - dice_coef_2: 0.9505 - val_loss: 0.0173 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.7429 - val_dice_coef_2: 0.9246\n",
      "Epoch 71/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0044 - dice_coef_0: 0.9995 - dice_coef_1: 0.9215 - dice_coef_2: 0.9541 - val_loss: 0.0283 - val_dice_coef_0: 0.9976 - val_dice_coef_1: 0.6335 - val_dice_coef_2: 0.7928\n",
      "Epoch 72/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0050 - dice_coef_0: 0.9995 - dice_coef_1: 0.9173 - dice_coef_2: 0.9453 - val_loss: 0.0469 - val_dice_coef_0: 0.9967 - val_dice_coef_1: 0.5511 - val_dice_coef_2: 0.7842\n",
      "Epoch 73/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0042 - dice_coef_0: 0.9995 - dice_coef_1: 0.9270 - dice_coef_2: 0.9595 - val_loss: 0.0301 - val_dice_coef_0: 0.9978 - val_dice_coef_1: 0.6768 - val_dice_coef_2: 0.9074\n",
      "Epoch 74/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0041 - dice_coef_0: 0.9996 - dice_coef_1: 0.9293 - dice_coef_2: 0.9535 - val_loss: 0.0342 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.6968 - val_dice_coef_2: 0.9160\n",
      "Epoch 75/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0043 - dice_coef_0: 0.9996 - dice_coef_1: 0.9273 - dice_coef_2: 0.9589 - val_loss: 0.0428 - val_dice_coef_0: 0.9977 - val_dice_coef_1: 0.7079 - val_dice_coef_2: 0.8929\n",
      "Epoch 76/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0039 - dice_coef_0: 0.9996 - dice_coef_1: 0.9401 - dice_coef_2: 0.9589 - val_loss: 0.0320 - val_dice_coef_0: 0.9979 - val_dice_coef_1: 0.7110 - val_dice_coef_2: 0.8900\n",
      "Epoch 77/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0039 - dice_coef_0: 0.9996 - dice_coef_1: 0.9255 - dice_coef_2: 0.9581 - val_loss: 0.0303 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.7121 - val_dice_coef_2: 0.8952\n",
      "Epoch 78/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0039 - dice_coef_0: 0.9996 - dice_coef_1: 0.9214 - dice_coef_2: 0.9596 - val_loss: 0.0452 - val_dice_coef_0: 0.9975 - val_dice_coef_1: 0.6817 - val_dice_coef_2: 0.8830\n",
      "Epoch 79/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9315 - dice_coef_2: 0.9623 - val_loss: 0.0299 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.7335 - val_dice_coef_2: 0.9290\n",
      "Epoch 80/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0039 - dice_coef_0: 0.9996 - dice_coef_1: 0.9383 - dice_coef_2: 0.9530 - val_loss: 0.0462 - val_dice_coef_0: 0.9972 - val_dice_coef_1: 0.6099 - val_dice_coef_2: 0.9031\n",
      "Epoch 81/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9382 - dice_coef_2: 0.9593 - val_loss: 0.0358 - val_dice_coef_0: 0.9976 - val_dice_coef_1: 0.6667 - val_dice_coef_2: 0.9405\n",
      "Epoch 82/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9293 - dice_coef_2: 0.9556 - val_loss: 0.0559 - val_dice_coef_0: 0.9972 - val_dice_coef_1: 0.6602 - val_dice_coef_2: 0.8649\n",
      "Epoch 83/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9340 - dice_coef_2: 0.9597 - val_loss: 0.0619 - val_dice_coef_0: 0.9960 - val_dice_coef_1: 0.6550 - val_dice_coef_2: 0.9394\n",
      "Epoch 84/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9228 - dice_coef_2: 0.9597 - val_loss: 0.0647 - val_dice_coef_0: 0.9960 - val_dice_coef_1: 0.6582 - val_dice_coef_2: 0.8767\n",
      "Epoch 85/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9361 - dice_coef_2: 0.9606 - val_loss: 0.0430 - val_dice_coef_0: 0.9974 - val_dice_coef_1: 0.6290 - val_dice_coef_2: 0.8530\n",
      "Epoch 86/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0038 - dice_coef_0: 0.9996 - dice_coef_1: 0.9230 - dice_coef_2: 0.9606 - val_loss: 0.0504 - val_dice_coef_0: 0.9971 - val_dice_coef_1: 0.6599 - val_dice_coef_2: 0.8337\n",
      "Epoch 87/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0046 - dice_coef_0: 0.9995 - dice_coef_1: 0.9249 - dice_coef_2: 0.9510 - val_loss: 0.0134 - val_dice_coef_0: 0.9990 - val_dice_coef_1: 0.7862 - val_dice_coef_2: 0.9003\n",
      "Epoch 88/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0039 - dice_coef_0: 0.9996 - dice_coef_1: 0.9292 - dice_coef_2: 0.9585 - val_loss: 0.0121 - val_dice_coef_0: 0.9990 - val_dice_coef_1: 0.8475 - val_dice_coef_2: 0.9327\n",
      "Epoch 89/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0037 - dice_coef_0: 0.9996 - dice_coef_1: 0.9371 - dice_coef_2: 0.9583 - val_loss: 0.0110 - val_dice_coef_0: 0.9991 - val_dice_coef_1: 0.8778 - val_dice_coef_2: 0.9556\n",
      "Epoch 90/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0037 - dice_coef_0: 0.9996 - dice_coef_1: 0.9315 - dice_coef_2: 0.9602 - val_loss: 0.0125 - val_dice_coef_0: 0.9990 - val_dice_coef_1: 0.8160 - val_dice_coef_2: 0.9220\n",
      "Epoch 91/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0037 - dice_coef_0: 0.9996 - dice_coef_1: 0.9418 - dice_coef_2: 0.9567 - val_loss: 0.0110 - val_dice_coef_0: 0.9991 - val_dice_coef_1: 0.8767 - val_dice_coef_2: 0.9377\n",
      "Epoch 92/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0039 - dice_coef_0: 0.9996 - dice_coef_1: 0.9125 - dice_coef_2: 0.9556 - val_loss: 0.0180 - val_dice_coef_0: 0.9986 - val_dice_coef_1: 0.7584 - val_dice_coef_2: 0.9298\n",
      "Epoch 93/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0036 - dice_coef_0: 0.9996 - dice_coef_1: 0.9295 - dice_coef_2: 0.9634 - val_loss: 0.0165 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.7534 - val_dice_coef_2: 0.9410\n",
      "Epoch 94/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0037 - dice_coef_0: 0.9996 - dice_coef_1: 0.9332 - dice_coef_2: 0.9611 - val_loss: 0.0216 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.7646 - val_dice_coef_2: 0.9309\n",
      "Epoch 95/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0046 - dice_coef_0: 0.9995 - dice_coef_1: 0.9270 - dice_coef_2: 0.9533 - val_loss: 0.0215 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.6962 - val_dice_coef_2: 0.9201\n",
      "Epoch 96/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0037 - dice_coef_0: 0.9996 - dice_coef_1: 0.9467 - dice_coef_2: 0.9593 - val_loss: 0.0256 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.6448 - val_dice_coef_2: 0.9132\n",
      "Epoch 97/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0036 - dice_coef_0: 0.9996 - dice_coef_1: 0.9335 - dice_coef_2: 0.9606 - val_loss: 0.0288 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.6519 - val_dice_coef_2: 0.9235\n",
      "Epoch 98/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0037 - dice_coef_0: 0.9996 - dice_coef_1: 0.9398 - dice_coef_2: 0.9647 - val_loss: 0.0378 - val_dice_coef_0: 0.9977 - val_dice_coef_1: 0.6577 - val_dice_coef_2: 0.8888\n",
      "Epoch 99/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0036 - dice_coef_0: 0.9996 - dice_coef_1: 0.9430 - dice_coef_2: 0.9595 - val_loss: 0.0261 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.7033 - val_dice_coef_2: 0.8994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "389/389 [==============================] - 57s 146ms/step - loss: 0.0036 - dice_coef_0: 0.9996 - dice_coef_1: 0.9322 - dice_coef_2: 0.9617 - val_loss: 0.0258 - val_dice_coef_0: 0.9981 - val_dice_coef_1: 0.6384 - val_dice_coef_2: 0.8772\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "    # source, target = data_convert()\n",
    "\n",
    "    source, target = dataload()\n",
    "    print(source.shape, target.shape)\n",
    "    \n",
    "    model = build_unet()\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(source, target, epochs=100, validation_split=0.2, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
